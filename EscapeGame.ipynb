{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EscapeGame.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUz7FUzFjKDZiO+CWUs/sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjmcfadyen/escape-game/blob/pythonSim/EscapeGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating behaviour in an escape game"
      ],
      "metadata": {
        "id": "BIybJ3Cp3bAs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BuhbYnvNyK9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from copy import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Environment\n",
        "\n",
        "The environment will have:\n",
        "*   A transition matrix (which you need to define manually)\n",
        "*   The current position of the exits, goal state, and predator (these are empty by default)\n",
        "\n",
        "The environment can:\n",
        "*   Show you a map using `showgrid`\n",
        "*   Set the position of the agent, predator, and exits using `setpos`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IEc3uYDS3zAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Environment:\n",
        "\n",
        "  # Initiate a 9-state grid world\n",
        "  def __init__(self):\n",
        "\n",
        "    # Set up grid\n",
        "    self.grid         = np.empty((3,3)) # 3 x 3 matrix\n",
        "    self.nStates      = np.prod(self.grid.shape)\n",
        "    self.coords       = np.indices(self.grid.shape).reshape(2,self.nStates)\n",
        "\n",
        "    self.coords       = np.flipud(self.coords) # swap x and y axes for plotting purposes\n",
        "    self.coords[1,:]  = np.flip(self.coords[1,:]) # invert y axis so that states 1, 2, 3, etc. are at the top of the plot\n",
        "\n",
        "    self.T      = np.zeros((self.nStates,self.nStates)) # transition matrix\n",
        "    self.T[0,1] = 1 # state 1 to state 2\n",
        "    self.T[0,3] = 1 # state 1 to state 4\n",
        "    self.T[1,0] = 1 # state 2 to state 1\n",
        "    self.T[1,4] = 1 # state 2 to state 5\n",
        "    self.T[1,2] = 1 # state 2 to state 3\n",
        "    self.T[2,1] = 1 # state 3 to state 2\n",
        "    self.T[2,5] = 1 # state 3 to state 6\n",
        "    self.T[3,0] = 1 # state 4 to state 1\n",
        "    self.T[3,6] = 1 # state 4 to state 7\n",
        "    self.T[4,1] = 1 # state 5 to state 2\n",
        "    self.T[4,7] = 1 # state 5 to state 8\n",
        "    self.T[5,8] = 1 # state 6 to state 9\n",
        "    self.T[6,7] = 1 # state 7 to state 8\n",
        "    self.T[6,3] = 1 # state 7 to state 4\n",
        "    self.T[7,8] = 1 # state 8 to state 9\n",
        "    self.T[7,4] = 1 # state 8 to state 5\n",
        "    self.T[8,5] = 1 # state 9 to state 6\n",
        "    self.T[8,7] = 1 # state 9 to state 8\n",
        "\n",
        "    # Set up defaults\n",
        "    self.agentpos     = None\n",
        "    self.predatorpos  = None\n",
        "    self.exits        = None\n",
        "    self.goal         = None\n",
        "    self.setpos()\n",
        "\n",
        "  def setpos(self,agent=None,predator=None,exits=None,goal=None):\n",
        "\n",
        "    # assumes the input is a state number starting from 1, and converts this to starting from 0\n",
        "    if agent is not None:\n",
        "      self.agentpos     = agent-1\n",
        "    if predator is not None:\n",
        "      self.predatorpos  = predator-1\n",
        "    if exits is not None:\n",
        "      self.exits        = [x - 1 for x in exits]\n",
        "    if goal is not None:\n",
        "      self.goal         = goal-1\n"
      ],
      "metadata": {
        "id": "zIbxKrc9OVYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The agent\n",
        "\n",
        "By default, the agent is a simple Q learner. \n",
        "\n",
        "### Q Learner\n",
        "The agent can navigate the environment by selecting which state to go to next. Thus, there are as many actions as there are states. Successful navigation requires selecting states in the correct order (i.e., selecting states that transition to each other). For a model-free learner to successfully transition without any knowledge of the state space, the chances of selecting a valid action sequence is 1 in 6561 (~0.02% chance). Thus, we restricted action selection to only valid transitions.\n",
        "\n",
        "### Successor representation\n",
        "\n",
        "\n",
        "### Model-based"
      ],
      "metadata": {
        "id": "YtbT0abGq7iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "  # ------------------------------------------------------------------------------------------------------\n",
        "  # Initiate agent with specific model & parameters\n",
        "  # ------------------------------------------------------------------------------------------------------\n",
        "  def __init__(self,model=\"qlearner\",params={\"alpha\": 0.5, \"gamma\": 1, \"epsilon\": 0.05},speed=1):\n",
        "\n",
        "    # Set defaults\n",
        "    self.status = \"ready\"\n",
        "    self.model = model\n",
        "\n",
        "    self.plan = None\n",
        "    self.points = None\n",
        "\n",
        "    # Set up model-specific variables\n",
        "    if model==\"qlearner\":\n",
        "      \n",
        "      # set default parameters\n",
        "      self.alpha = 0.5\n",
        "      self.gamma = 1\n",
        "      self.epsilon = 0.05\n",
        "\n",
        "      # replace defaults with input\n",
        "      if \"alpha\" in params:\n",
        "        self.alpha = params[\"alpha\"]\n",
        "\n",
        "      if \"gamma\" in params:\n",
        "        self.gamma = params[\"gamma\"]\n",
        "\n",
        "      if \"epsilon\" in params:\n",
        "        self.epsilon = params[\"epsilon\"]\n",
        "\n",
        "      # create empty tables\n",
        "      self.Q = None\n",
        "      self.V = None\n",
        "      self.R = None\n",
        "\n",
        "    # Set agent move speed\n",
        "    self.speed = speed\n",
        "\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  # Have the agent check the environment and update it according to the \"play\" rules (e.g., value of rewards, etc.)\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  def checkEnv(self,play):\n",
        "    \n",
        "    # check current position\n",
        "    self.pos = play.env.agentpos\n",
        "\n",
        "    # get list of possible actions\n",
        "    self.actions = [i for i in range(play.env.nStates)]\n",
        "\n",
        "    # initialise tables for Q learner, if empty\n",
        "    if self.model==\"qlearner\":\n",
        "\n",
        "      if self.Q is None:\n",
        "        self.Q = np.zeros((play.env.nStates,play.env.nStates))\n",
        "\n",
        "      if self.V is None:\n",
        "        self.V = np.zeros(play.env.nStates)\n",
        "\n",
        "      if self.R is None:\n",
        "\n",
        "        self.R = np.zeros((play.env.nStates,play.env.nStates))\n",
        "\n",
        "        # if there is no \"goal\" state, the exits are rewarded\n",
        "        if play.env.goal is None:\n",
        "          exits = play.env.exits\n",
        "          for exit in exits:\n",
        "            idx = np.where(play.env.T[:,exit]==1)\n",
        "            self.R[idx,exit] = play.exitReward\n",
        "          self.terminals = exits\n",
        "        else: # otherwise, if there IS a goal state, this is the rewarded state\n",
        "          goal = play.env.goal\n",
        "          idx = np.where(play.env.T[:,goal]==1)\n",
        "          self.R[idx[0],goal] = play.goalReward\n",
        "          self.terminals = goal\n",
        "\n",
        "        # take note of predator, if present\n",
        "        if play.env.predatorpos is not None:\n",
        "          predator = play.env.predatorpos\n",
        "          idx = np.where(play.env.T[:,predator]==1)\n",
        "          self.R[idx[0],goal] = play.predatorPenalty\n",
        "          self.terminals = predator\n",
        "\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  # Decide which actions to take\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  def makePlan(self,play):\n",
        "  \n",
        "    if self.model==\"qlearner\":\n",
        "      \n",
        "      plan = [self.pos] # plan starts from current location\n",
        "      for step in range(play.nSteps):\n",
        "\n",
        "        # only select actions that have possible transitions, and aren't already in plan\n",
        "        laststep = plan[len(plan)-1]\n",
        "        reachable = np.array(np.where(play.env.T[laststep,:]==1)[0])\n",
        "        actions = np.setdiff1d(reachable,plan) # only select reachable states that aren't in plan\n",
        "\n",
        "        # choose an action either randomly (epsilon parameter) or according to Q table\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "          choice = np.random.choice(actions,1).astype(int)\n",
        "        else:\n",
        "          validQ = self.Q[self.pos,actions]\n",
        "          maxidx = actions[np.argwhere(validQ == np.amax(validQ))].flatten()\n",
        "          choice = np.random.choice(maxidx,1)\n",
        "\n",
        "        plan = np.append(plan,choice.astype(int))\n",
        "\n",
        "      self.plan = plan.astype(int)\n",
        "      print(\"Planned action sequence = \", [x + 1 for x in self.plan])\n",
        "\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  # Execute action sequence and log time\n",
        "  # ----------------------------------------------------------------------------------------------------------------\n",
        "  def executePlan(self,play):\n",
        "\n",
        "    trajectory  = []\n",
        "    timestamp   = []\n",
        "    reward      = 0\n",
        "    outcome     = None\n",
        "\n",
        "    currentPos  = self.plan[0]\n",
        "    step        = 0\n",
        "    moving      = True\n",
        "    while moving:\n",
        "      \n",
        "      step    = step+1\n",
        "      action  = self.plan[step]\n",
        "\n",
        "      # check if chosen state (action) can be transitioned to from current position\n",
        "      validTransition = play.env.T[currentPos,action]\n",
        "\n",
        "      if validTransition:\n",
        "\n",
        "        nextState   = action # here, the action IS the state that has been selected\n",
        "        trajectory  = np.append(trajectory,nextState)\n",
        "        timestamp   = np.append(timestamp,1/self.speed) # time (in \"seconds\") = distance (1 state) / agent speed (no. states per \"second\")\n",
        "\n",
        "        # check for any reward\n",
        "        reward = self.R[currentPos,action]\n",
        "\n",
        "        if self.model==\"qlearner\":\n",
        "\n",
        "          # update Q table\n",
        "          oldQ = self.Q[currentPos,action]\n",
        "          next_max = np.max(self.Q[nextState,:])\n",
        "          newQ = (1 - self.alpha) * oldQ + self.alpha * (reward + self.gamma * next_max)\n",
        "          self.Q[currentPos,action] = newQ\n",
        "\n",
        "        # upate agent position\n",
        "        currentPos = nextState\n",
        "\n",
        "        # check if agent should terminate plan (reached a goal or exit)\n",
        "        if currentPos == self.terminals:\n",
        "\n",
        "          if play.env.goal is not None and currentPos == play.env.goal:\n",
        "            print(\"Agent reached goal state\")\n",
        "            outcome = \"goal reached\"\n",
        "            self.status = \"working on goal\"\n",
        "            \n",
        "          elif currentPos == play.env.exits:\n",
        "            print(\"Agent reached exit\")\n",
        "            outcome = \"escaped\"\n",
        "\n",
        "          self.pos = currentPos\n",
        "          print(\"Agent pos = \",self.pos+1)\n",
        "          moving = False\n",
        "\n",
        "      else: # invalid transition...\n",
        "\n",
        "        print(\"Agent's plan was invalid\")\n",
        "\n",
        "        outcome = \"invalid plan\"\n",
        "        moving = False\n",
        "\n",
        "      if step >= len(self.plan)-1:\n",
        "        moving = False\n",
        "\n",
        "    output = {\"plan\": self.plan,\n",
        "              \"trajectory\": trajectory,\n",
        "              \"timestamp\": timestamp,\n",
        "              \"reward\": reward,\n",
        "              \"outcome\": outcome}\n",
        "    return(output)\n"
      ],
      "metadata": {
        "id": "i4k6iT1fq6_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The predator"
      ],
      "metadata": {
        "id": "KtxZEiZjaPHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Predator:\n",
        "\n",
        "  # -------------------------------------------------------------------------------\n",
        "  # Initiate\n",
        "  # -------------------------------------------------------------------------------\n",
        "  def __init__(self,speed=1.3333,location=[],probability=[],waitdistribution=\"uniform\",params=[0,2]):\n",
        "\n",
        "    # determine how quickly (moves per \"second\") the predator can move from one state to the next\n",
        "    self.speed = speed\n",
        "\n",
        "    # randomly give predator a spawn location (in numbers starting from 1, not 0), given probability\n",
        "    self.pos = None\n",
        "    if location is not None:\n",
        "      if probability is not None:\n",
        "        if len(location) != len(probability):\n",
        "          raise Exception(\"Number of locations must match probability vector\")\n",
        "        elif sum(probability) != 1:\n",
        "          raise Exception(\"Probability vector must sum to 1!\")\n",
        "        else:\n",
        "          self.pos = np.random.choice(location,1,False,probability).astype(int)[0] - 1\n",
        "      elif len(location) > 1:\n",
        "        raise Exception(\"More than one location given for predator without a probability vector!\")\n",
        "      else:\n",
        "        self.pos = location-1\n",
        "\n",
        "    # set probability of waiting before persuing agent\n",
        "    self.waitdistribution = waitdistribution\n",
        "    self.params = params\n",
        "\n",
        "  # ---------------------------------------------------------------------------------------------\n",
        "  # Have the predator follow the agent's trajectory (assumes predator is 1 move away from agent)\n",
        "  # ---------------------------------------------------------------------------------------------\n",
        "  def followAgent(self,play):\n",
        "\n",
        "      print(\"Predator is chasing agent...\")\n",
        "\n",
        "      # see how many \"seconds\" until predator catches up with agent\n",
        "      attackTime = round(1/(play.predator.speed-play.agent.speed))\n",
        "\n",
        "      # add any waiting time before predator starts persuit\n",
        "      waitTime = 0\n",
        "      if self.waitdistribution==\"uniform\":\n",
        "        if len(self.params) != 2:\n",
        "          raise Exception(\"Incorrect number of parameters given for uniform distribution\")\n",
        "        else:\n",
        "          waitTime = np.random.uniform(self.params[0],self.params[1],1)\n",
        "      waitTime = np.round(waitTime,1)\n",
        "      attackTime = np.round(attackTime + waitTime).astype(int)\n",
        "      \n",
        "      print(\"Predator waits \", waitTime, \" seconds before persuing agent...\")\n",
        "\n",
        "      # check if agent will have escaped before this time\n",
        "      exitloc = np.zeros(len(play.agent.plan),dtype=bool)\n",
        "      for exit in play.env.exits:\n",
        "        idx = np.where(play.agent.plan==exit)[0]\n",
        "        if idx.size > 0:\n",
        "          exitloc[idx] = True\n",
        "      \n",
        "      escape = False\n",
        "      if any(exitloc): # agent finds an exit\n",
        "        escapeTime = np.where(exitloc)[0][0]/play.agent.speed\n",
        "        if escapeTime < attackTime: # exit is close enough\n",
        "          escape = True\n",
        "  \n",
        "      return(escape,attackTime)\n",
        "\n"
      ],
      "metadata": {
        "id": "9jW9VJKSaQ-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The playthrough\n",
        "\n"
      ],
      "metadata": {
        "id": "FloSoqXyaIq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Play:\n",
        "\n",
        "  # -------------------------------------------------------------------------------\n",
        "  # Set defaults and initiate episodes\n",
        "  # -------------------------------------------------------------------------------\n",
        "  def __init__(self,env,agent,nEpisodes=10):\n",
        "\n",
        "    self.env   = env;\n",
        "    self.agent = agent;\n",
        "\n",
        "    # Point settings\n",
        "    self.predatorPenalty   = -1 # penalty for being caught by the predator\n",
        "    self.goalReward        =  1 # reward for getting to goal state\n",
        "    self.exitReward        =  1 # reward for making it to the exit\n",
        "\n",
        "    # Predator settings\n",
        "    self.predator          = Predator(location=[1,3,5],probability=[0.8,0.1,0.1])\n",
        "\n",
        "    # Planning settings\n",
        "    self.nSteps            = 4 # maximum number of actions that can be selected\n",
        "\n",
        "    # Start\n",
        "    self.episodes = []\n",
        "    self.run(nEpisodes)\n",
        "\n",
        "  # -------------------------------------------------------------------------------\n",
        "  # Run through all episodes\n",
        "  # -------------------------------------------------------------------------------\n",
        "  def run(self,nEpisodes):\n",
        "\n",
        "    origEnv   = copy(self.env)\n",
        "    origAgent = copy(self.agent)\n",
        "\n",
        "    for episode in range(nEpisodes):\n",
        "\n",
        "      print(\"---------------------------------------\")\n",
        "      print(\"EPISODE \",episode,\" of \", nEpisodes)\n",
        "      print(\"---------------------------------------\")\n",
        "\n",
        "      # reset environment\n",
        "      self.env          = copy(origEnv);\n",
        "      self.agent        = copy(origAgent);\n",
        "      self.agent.status = \"ready\"\n",
        "      print(\"agent pos = \", self.env.agentpos)\n",
        "      print(\"predator pos = \", self.env .predatorpos)\n",
        "\n",
        "      # run episode\n",
        "      complete = False\n",
        "      while not complete:\n",
        "\n",
        "        # update the agent about the environment\n",
        "        self.agent.checkEnv(self)\n",
        "\n",
        "        # --------------------------------------\n",
        "        # have the agent go to the goal location\n",
        "        # --------------------------------------\n",
        "        print(\"agent status = \",self.agent.status)\n",
        "        if self.agent.status == \"ready\":\n",
        "          \n",
        "          # have agent construct a plan\n",
        "          self.agent.makePlan(self)\n",
        "\n",
        "          # see if the agent can successfully execute the plan to the goal location\n",
        "          output1 = self.agent.executePlan(self)\n",
        "          output1[\"episode\"] = episode\n",
        "          output1[\"task\"] = \"go to goal\"\n",
        "\n",
        "          self.episodes.append([output1])\n",
        "\n",
        "          # -------------------------------------------------------------------\n",
        "          # if the agent got to the goal location, make them escape to an exit\n",
        "          # -------------------------------------------------------------------\n",
        "          if self.agent.status == \"working on goal\":\n",
        "\n",
        "            # put predator in environment, update agent location, and remove goal\n",
        "            self.env.setpos(predator=self.predator.pos+1, agent=self.agent.pos+1, goal=None)\n",
        "            print(\"Predator assigned to state \",self.predator.pos+1)\n",
        "\n",
        "            # have agent check environment to update reward function\n",
        "            self.agent.checkEnv(self)\n",
        "\n",
        "            # have agent construct a plan\n",
        "            self.agent.status = \"escaping\"\n",
        "            self.agent.makePlan(self)\n",
        "\n",
        "            # see if the agent can successfully execute the plan to the nearest exit\n",
        "            chaseOutcome = self.predator.followAgent(self)\n",
        "            escape = chaseOutcome[0]\n",
        "            attackTime = chaseOutcome[1]\n",
        "\n",
        "            if escape: # if agent can escape, then execute plan\n",
        "              output2 = self.agent.executePlan(self)\n",
        "              output2[\"outcome\"] = \"escaped\"\n",
        "              print(\"Agent escaped\")\n",
        "            else:\n",
        "              output2 = self.agent.executePlan(self)\n",
        "              output2[\"outcome\"] = \"died\"\n",
        "              output2[\"trajectory\"] = output2[\"trajectory\"][range(attackTime[0]-1)] # remove any \"trajectory\" that occurs after attack\n",
        "              print(\"Agent died\")\n",
        "            \n",
        "            output2[\"episode\"] = episode\n",
        "            output2[\"task\"] = \"escape\"\n",
        "\n",
        "            self.episodes[episode] = [output1,output2]\n",
        "\n",
        "        complete = True\n",
        "\n",
        "      # Plot episode if successful\n",
        "      for part in range(len(self.episodes[episode])):\n",
        "        fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "        thistitle = \"Agent = \" + self.agent.model + \", episode \" + str(len(self.episodes))\n",
        "        fig.suptitle(thistitle)\n",
        "        self.showgrid(ax1)\n",
        "        if self.agent.model==\"qlearner\":\n",
        "          self.showQ(ax2)\n"
      ],
      "metadata": {
        "id": "L7hH4iPLvMb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation"
      ],
      "metadata": {
        "id": "Owzqv5StXoGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "# Create map\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "def showgrid(env):\n",
        "\n",
        "  # Draw lines connecting each state\n",
        "  for state in range(env.nStates):\n",
        "    transitions = np.flatnonzero(env.T[state,:]==1)\n",
        "    for transition in transitions:\n",
        "      x = [env.coords[0,state],env.coords[0,transition]]\n",
        "      y = [env.coords[1,state],env.coords[1,transition]]\n",
        "      ax.plot(x,y,color=\"black\",zorder=1)\n",
        "\n",
        "  # Draw circles with numbers for each state\n",
        "  for state in range(env.nStates):\n",
        "\n",
        "    if np.any(np.in1d(state, env.exits)):\n",
        "      statecolor = \"#20FB61\"\n",
        "    elif np.any(np.in1d(state, env.predatorpos)):\n",
        "      statecolor = \"#F93D40\" \n",
        "    elif np.any(np.in1d(state, env.goal)):\n",
        "      statecolor = \"#0DB0FC\" \n",
        "    else:\n",
        "      statecolor = \"gray\"\n",
        "\n",
        "    if np.any(np.in1d(state, env.agentpos)):\n",
        "      linecolor = \"#A736F8\"\n",
        "    else:\n",
        "      linecolor = \"none\"\n",
        "\n",
        "    ax.scatter(env.coords[0,state],env.coords[1,state],500,zorder=2,color=statecolor,edgecolor=linecolor,linewidth=3)\n",
        "    ax.text(env.coords[0,state],env.coords[1,state],str(state+1))\n",
        "\n",
        "  # Find one-way transitions\n",
        "  for state in range(env.nStates):\n",
        "    transitions = np.flatnonzero(self.env.T[state,:]==1)\n",
        "    for transition in transitions:\n",
        "      reciprocal = self.env.T[transition,state]==1\n",
        "      if not reciprocal:\n",
        "        shift = self.env.coords[:,transition] - self.env.coords[:,state]\n",
        "        if shift[0] != 0:\n",
        "          width = 0.1\n",
        "          height = 0.3\n",
        "          x = self.env.coords[0,state] - 0.04 + 0.2*shift[0]\n",
        "          y = self.env.coords[1,state] - 0.15 + 0.2*shift[1]\n",
        "        else:\n",
        "          width = 0.3\n",
        "          height = 0.1\n",
        "          x = self.env.coords[0,state] - 0.15 + 0.2*shift[0]\n",
        "          y = self.env.coords[1,state] - 0.06 + 0.2*shift[1]\n",
        "        ax.add_patch(patches.Rectangle((x,y),width,height,zorder=4,color=\"orange\"))\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "# Make a movie of agents navigating each episode\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "def movieEpisode(episode):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "# Plot grid world\n",
        "# ----------------------------------------------------------------------------------------------------------------------------\n",
        "def showgrid(self,ax=None):\n",
        "\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  # Draw lines connecting each state\n",
        "  for state in range(self.env.nStates):\n",
        "    transitions = np.flatnonzero(self.env.T[state,:]==1)\n",
        "    for transition in transitions:\n",
        "      x = [self.env.coords[0,state],self.env.coords[0,transition]]\n",
        "      y = [self.env.coords[1,state],self.env.coords[1,transition]]\n",
        "      ax.plot(x,y,color=\"black\",zorder=1)\n",
        "\n",
        "  # Draw circles with numbers for each state\n",
        "  for state in range(self.env.nStates):\n",
        "\n",
        "    if np.any(np.in1d(state, self.env.exits)):\n",
        "      statecolor = \"#20FB61\"\n",
        "    elif np.any(np.in1d(state, self.env.predatorpos)):\n",
        "      statecolor = \"#F93D40\" \n",
        "    elif np.any(np.in1d(state, self.env.goal)):\n",
        "      statecolor = \"#0DB0FC\" \n",
        "    else:\n",
        "      statecolor = \"gray\"\n",
        "\n",
        "    if np.any(np.in1d(state, self.env.agentpos)):\n",
        "      linecolor = \"#A736F8\"\n",
        "    else:\n",
        "      linecolor = \"none\"\n",
        "\n",
        "    ax.scatter(self.env.coords[0,state],self.env.coords[1,state],500,zorder=2,color=statecolor,edgecolor=linecolor,linewidth=3)\n",
        "    ax.text(self.env.coords[0,state],self.env.coords[1,state],str(state+1))\n",
        "\n",
        "  # Find one-way transitions\n",
        "  for state in range(self.env.nStates):\n",
        "    transitions = np.flatnonzero(self.env.T[state,:]==1)\n",
        "    for transition in transitions:\n",
        "      reciprocal = self.env.T[transition,state]==1\n",
        "      if not reciprocal:\n",
        "        shift = self.env.coords[:,transition] - self.env.coords[:,state]\n",
        "        if shift[0] != 0:\n",
        "          width = 0.1\n",
        "          height = 0.3\n",
        "          x = self.env.coords[0,state] - 0.04 + 0.2*shift[0]\n",
        "          y = self.env.coords[1,state] - 0.15 + 0.2*shift[1]\n",
        "        else:\n",
        "          width = 0.3\n",
        "          height = 0.1\n",
        "          x = self.env.coords[0,state] - 0.15 + 0.2*shift[0]\n",
        "          y = self.env.coords[1,state] - 0.06 + 0.2*shift[1]\n",
        "        ax.add_patch(patches.Rectangle((x,y),width,height,zorder=4,color=\"orange\"))\n",
        "\n",
        "  # Draw agent's plan vs trajectory (if present)\n",
        "  idx = len(self.episodes)-1\n",
        "\n",
        "  plan = self.episodes[idx][0][\"plan\"]\n",
        "  if plan.size > 0:\n",
        "    for step in range(len(plan)):\n",
        "      if step==0:\n",
        "        x = [self.env.coords[0,self.env.agentpos],self.env.coords[0,plan[step]]]\n",
        "        y = [self.env.coords[1,self.env.agentpos],self.env.coords[1,plan[step]]]\n",
        "      else:\n",
        "        x = [self.env.coords[0,plan[step-1]],self.env.coords[0,plan[step]]]\n",
        "        y = [self.env.coords[1,plan[step-1]],self.env.coords[1,plan[step]]]\n",
        "      ax.plot(x,y,color=\"#A736F8\",linestyle=\"dashed\",linewidth=4)\n",
        "\n",
        "  # Draw invisible bounding box\n",
        "  padding = 0.5;\n",
        "  x = [self.env.coords[0,:].min()-padding, self.env.coords[0,:].min()-padding, self.env.coords[0,:].max()+padding, self.env.coords[0,:].max()+padding]\n",
        "  y = [self.env.coords[1,:].min()-padding, self.env.coords[1,:].max()+padding, self.env.coords[1,:].max()+padding, self.env.coords[1,:].min()-padding]\n",
        "  ax.plot(x,y,alpha=0)\n",
        "\n",
        "  # Set bounds of plot\n",
        "  ax.set_aspect('equal', 'box')\n",
        "  ax.axis('off')\n",
        "  \n",
        "  return(ax)\n",
        "\n",
        "# Plot heatmap of Q values\n",
        "def showQ(self,ax=None):\n",
        "\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  ax.imshow(self.agent.Q)\n",
        "\n",
        "  # Major ticks\n",
        "  ax.set_xticks(np.arange(0, self.env.nStates, 1))\n",
        "  ax.set_yticks(np.arange(0, self.env.nStates, 1))\n",
        "\n",
        "  # Labels for major ticks\n",
        "  ax.set_xticklabels(np.arange(1, self.env.nStates+1, 1))\n",
        "  ax.set_yticklabels(np.arange(1, self.env.nStates+1, 1))\n",
        "\n",
        "  # Minor ticks\n",
        "  ax.set_xticks(np.arange(-.5, self.env.nStates, 1), minor=True)\n",
        "  ax.set_yticks(np.arange(-.5, self.env.nStates, 1), minor=True)\n",
        "\n",
        "  # Gridlines based on minor ticks\n",
        "  ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
        "\n",
        "  # lables\n",
        "  ax.set_ylabel(\"Position\")\n",
        "  ax.set_xlabel(\"Action\")"
      ],
      "metadata": {
        "id": "qXQYOPgsXp5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment set up\n",
        "level = Environment()\n",
        "level.setpos(agent=6,exits=[6,7],goal=2)\n",
        "\n",
        "# Play out level\n",
        "agent = Agent(\"qlearner\")\n",
        "play = Play(level,agent,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 932
        },
        "id": "i1DzbHLa7MQf",
        "outputId": "5524c1e9-4060-4f3e-a6a0-8faf15abb6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "EPISODE  0  of  1\n",
            "---------------------------------------\n",
            "agent pos =  5\n",
            "predator pos =  None\n",
            "agent status =  ready\n",
            "Planned action sequence =  [6, 9, 8, 5, 2]\n",
            "Agent reached goal state\n",
            "Agent pos =  2\n",
            "Predator assigned to state  1\n",
            "Planned action sequence =  [2, 3, 6, 9, 8]\n",
            "Predator is chasing agent...\n",
            "Predator waits  [0.]  seconds before persuing agent...\n",
            "Agent escaped\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-311-ec08ea9c2aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Play out level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qlearner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-310-db91161f5e57>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, agent, nEpisodes)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnEpisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;31m# -------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-310-db91161f5e57>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, nEpisodes)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"qlearner\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mshowgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;31m# ----------------------------------------------------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'showgrid' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}